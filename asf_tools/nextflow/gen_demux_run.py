"""
Function class for managing CLI operation
"""

import csv
import logging
import os
import stat

from asf_tools.api.clarity.clarity_helper_lims import ClarityHelperLims
from asf_tools.illumina.illumina_utils import extract_illumina_runid_frompath
from asf_tools.io.data_management import DataManagement, DataTypeMode
from asf_tools.io.utils import list_directory_names
from asf_tools.nextflow.utils import create_sbatch_header


log = logging.getLogger(__name__)

PERM777 = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH

PERM666 = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP | stat.S_IROTH | stat.S_IWOTH


class GenDemuxRun:
    """
    Generates a run folder for the demux pipeline and associated support files
    including a run script and default samplesheet
    """

    def __init__(
        self,
        source_dir,
        target_dir,
        mode,
        pipeline_dir,
        nextflow_cache,
        nextflow_work,
        container_cache,
        runs_dir,
        use_api,
        contains=None,
        samplesheet_only=False,
        nextflow_version=None,
    ) -> None:
        self.source_dir = source_dir
        self.target_dir = target_dir
        self.mode = mode
        self.pipeline_dir = pipeline_dir
        self.nextflow_cache = nextflow_cache
        self.nextflow_work = nextflow_work
        self.container_cache = container_cache
        self.runs_dir = runs_dir
        self.use_api = use_api
        self.contains = contains
        self.samplesheet_only = samplesheet_only
        self.nextflow_version = nextflow_version

    def cli_run(self):
        """
        Run function from CLI
        """

        log.debug("Scanning run folder")

        # Init
        dm = DataManagement()

        # Pull list of directory names
        source_dir_names = set(list_directory_names(self.source_dir))
        target_dir_names = set(list_directory_names(self.target_dir))

        # Check for a sequencing_summary file in source directory to show that the run is completed
        completed_runs = []
        for run_name in source_dir_names:
            run_dir = os.path.join(self.source_dir, run_name)
            completed_file_exists = dm.check_ont_sequencing_run_complete(run_dir)
            if completed_file_exists:
                completed_runs.append(run_name)
            else:
                log.debug(f"Skipping {run_name}: no summary file detected, run not completed.")
        source_dir_names = set(completed_runs)
        log.info(f"Found {len(completed_runs)} completed runs")

        # Get diff
        if self.samplesheet_only is False:
            dir_diff = source_dir_names - target_dir_names
            log.info(f"Found {len(dir_diff)} new run folders")
        else:
            dir_diff = target_dir_names
            log.info(f"Found {len(dir_diff)} existing run folders")

        # Filter for contains
        if self.contains is not None:
            dir_diff = [run for run in dir_diff if self.contains in run]
            log.info(f"Found {len(dir_diff)} new run folders after filtering for {self.contains}")

        # Process runs
        for run_name in dir_diff:
            try:
                self.process_cli_run(run_name, self.mode)
            except Exception as e:  # pylint: disable=broad-exception-caught
                # Catch any possible errors generated by the connection to the API, generating the samplesheet or generating the run script
                log.error(f"Error for {run_name}: {e}")

        return 0

    def process_cli_run(self, run_name: str, mode: DataTypeMode):
        """
        Per run processing
        """

        # Init
        cl = ClarityHelperLims()

        log.info(f"Processing: {run_name}")
        sample_count = 0

        # Create folder
        folder_path = os.path.join(self.target_dir, run_name)
        if self.samplesheet_only is False and not os.path.exists(folder_path):
            os.makedirs(folder_path)

        # Samplesheet path
        samplesheet_path = os.path.join(folder_path, "samplesheet.csv")

        if self.use_api is False:
            # Write default samplesheet
            sample_count = 1
            with open(samplesheet_path, "w", encoding="UTF-8") as file:
                file.write("id,sample_name,group,user,project_id,project_limsid,project_type,reference_genome,data_analysis_type,barcode\n")
                file.write("sample_01,sample_01,asf,no_name,no_proj,no_lims_proj,no_type,no_ref,no_analysis,unclassified\n")
        if self.use_api is True:
            # Get samplesheet from API
            sample_dict = self.get_samplesheet(cl, run_name, mode)

            # Write samplesheet
            sample_count, samplesheet = self.create_samplesheet(sample_dict)
            with open(samplesheet_path, "w", encoding="UTF-8") as file:
                for line in samplesheet:
                    file.write(line + "\n")

        # Set 666 for the samplesheet
        os.chmod(samplesheet_path, PERM666)

        # Generate and write sbatch script
        if self.samplesheet_only is False:
            # Detect muliplexed samples
            bc_parse_pos = -1
            if sample_count > 1:
                bc_parse_pos = 2

            # Extract pipeline parameters
            pipeline_params = self.extract_pipeline_params(cl, samplesheet_path)

            # Create sbatch script
            sbatch_script = self.create_ont_sbatch_text(run_name, pipeline_params, bc_parse_pos)
            sbatch_script_path = os.path.join(folder_path, "run_script.sh")
            with open(sbatch_script_path, "w", encoding="UTF-8") as file:
                file.write(sbatch_script)

            # Set 777 for the run script
            os.chmod(sbatch_script_path, PERM777)

    def get_samplesheet(self, api, run_name: str, mode: DataTypeMode) -> dict:
        """
        Get samplesheet from API
        """
        # Get samplesheet from API

        # Check mode and set the appropriate check function
        if mode == DataTypeMode.ONT:
            sample_dict = api.collect_samplesheet_info(run_name)
        elif mode == DataTypeMode.ILLUMINA:
            # extract illumina RunId/flowcell name, then run check function
            run_dir = os.path.join(self.source_dir, run_name)
            illumina_run_name = extract_illumina_runid_frompath(run_dir, "RunInfo.xml")
            sample_dict = api.collect_samplesheet_info(illumina_run_name)
        else:
            sample_dict = api.collect_samplesheet_info(run_name)
            log.warning(f"Mode selected: {mode}. General samplesheet created.")

        return sample_dict

    def create_samplesheet(self, sample_dict: dict) -> list:
        """
        Create a samplesheet as a list of strings
        """
        samplesheet = []
        count = 0
        samplesheet.append("id,sample_name,group,user,project_id,project_limsid,project_type,reference_genome,data_analysis_type,barcode")
        for key, value in sample_dict.items():
            # Convert all None values to "" for CSV
            clean_dict = {key: ("" if value is None or value == "None" else value) for key, value in value.items()}
            barcode = "unclassified"
            if "barcode" in value and clean_dict["barcode"] != "":
                barcode = clean_dict["barcode"]
            samplesheet.append(
                f"{key},{clean_dict['sample_name']},{clean_dict['group']},{clean_dict['user']},{clean_dict['project_id']},{clean_dict['project_limsid']},{clean_dict['project_type']},{clean_dict['reference_genome']},{clean_dict['data_analysis_type']},{barcode}"
            )
            count += 1

        return count, samplesheet

    def extract_pipeline_params(self, api, samplesheet_csv) -> dict:
        """
        Extracts the first project IDs from a CSV file and retrieves the corresponding pipeline parameters.
        """
        params_dict = {}
        with open(samplesheet_csv, mode="r", encoding="utf-8") as file:
            reader = csv.DictReader(file)

            # Check there's a column for "project_id"
            if "project_id" not in reader.fieldnames:
                return params_dict

            # Extract the first project ID
            first_row = next(reader, None)
            proj_id = first_row["project_id"]
            params_dict = api.get_pipeline_params(proj_id, "pipeline params", "=")

        return params_dict

    def create_ont_sbatch_text(self, run_name: str, pipeline_params_dict: dict, parse_pos: int = -1) -> str:
        """Creates an sbatch script from a template and returns the text

        Returns:
            str: Script as a string
        """

        # Create sbatch header
        header_str = create_sbatch_header(self.nextflow_version)

        # Create NXF_HOME string
        nxf_home = ""
        if self.nextflow_cache != "":
            nxf_home = f'export NXF_HOME="{self.nextflow_cache}"'

        # Create the bash script template with placeholders
        bash_script = f"""#!/bin/sh

#SBATCH --partition=ncpu
#SBATCH --job-name=asf_nanopore_demux_{run_name}
#SBATCH --mem=4G
#SBATCH -n 1
#SBATCH --time=168:00:00
#SBATCH --output=run.o
#SBATCH --error=run.o
#SBATCH --res=asf

{header_str}

{nxf_home}
export NXF_WORK="{self.nextflow_work}"
export NXF_SINGULARITY_CACHEDIR="{self.container_cache}"

nextflow run {self.pipeline_dir} \\
  -resume \\
  -profile crick,genomics \\
  --monochrome_logs \\
  --samplesheet ./samplesheet.csv \\
  --run_dir {os.path.join(self.runs_dir, run_name)} \\
  --dorado_model sup"""
        # Add pipeline params
        if pipeline_params_dict is not None:
            print(pipeline_params_dict)
            for param in pipeline_params_dict:
                for key, value in param.items():
                    bash_script += f"  --{key} {value}\n"

        # Add parse pos if it is not -1
        if parse_pos != -1:
            bash_script += f" \\\n  --dorado_bc_parse_pos {parse_pos}\n"
        else:
            bash_script += "\n"

        return bash_script
