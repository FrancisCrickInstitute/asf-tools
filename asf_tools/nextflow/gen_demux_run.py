"""
Function class for managing CLI operation
"""

import logging
import os

from asf_tools.api.clarity.clarity_helper_lims import ClarityHelperLims
from asf_tools.illumina.illumina_utils import IlluminaUtils
from asf_tools.io.data_management import DataManagement, DataTypeMode
from asf_tools.io.storage_interface import StorageInterface, InterfaceType
from asf_tools.nextflow.utils import create_sbatch_header


log = logging.getLogger(__name__)

class GenDemuxRun:
    """
    Generates a run folder for the demux pipeline and associated support files
    including a run script and default samplesheet
    """

    def __init__(
        self,
        source_dir,
        target_dir,
        mode,
        pipeline_dir,
        nextflow_cache,
        nextflow_work,
        container_cache,
        runs_dir,
        use_api,
        contains=None,
        samplesheet_only=False,
        nextflow_version=None,
        file_system=InterfaceType.LOCAL,
    ) -> None:
        self.source_dir = source_dir
        self.target_dir = target_dir
        self.mode = mode
        self.pipeline_dir = pipeline_dir
        self.nextflow_cache = nextflow_cache
        self.nextflow_work = nextflow_work
        self.container_cache = container_cache
        self.runs_dir = runs_dir
        self.use_api = use_api
        self.contains = contains
        self.samplesheet_only = samplesheet_only
        self.nextflow_version = nextflow_version
        self.file_system = file_system

        self.storage_interface = StorageInterface(self.file_system)
        self.data_mgr = DataManagement(self.storage_interface)


    def run(self):
        """
        Run function from CLI
        """

        log.debug("Scanning run folder")

        # Pull list of directory names
        source_dir_names = set(self.storage_interface.list_directories_with_links(self.source_dir))
        target_dir_names = set(self.storage_interface.list_directories_with_links(self.target_dir))

        # Check for a sequencing_summary file in source directory to show that the run is completed
        completed_runs = []
        for run_name in source_dir_names:
            run_dir = os.path.join(self.source_dir, run_name)
            completed_file_exists = self.data_mgr.check_ont_sequencing_run_complete(run_dir)
            if completed_file_exists:
                completed_runs.append(run_name)
            else:
                log.debug(f"Skipping {run_name}: no summary file detected, run not completed.")
        source_dir_names = set(completed_runs)
        log.info(f"Found {len(completed_runs)} completed runs")

        # Get diff
        if self.samplesheet_only is False:
            dir_diff = source_dir_names - target_dir_names
            log.info(f"Found {len(dir_diff)} new run folders")
        else:
            dir_diff = target_dir_names
            log.info(f"Found {len(dir_diff)} existing run folders")

        # Filter for contains
        if self.contains is not None:
            dir_diff = [run for run in dir_diff if self.contains in run]
            log.info(f"Found {len(dir_diff)} new run folders after filtering for {self.contains}")

        # Process runs
        for run_name in dir_diff:
            try:
                self.process_run(run_name, self.mode)
            except Exception as e:  # pylint: disable=broad-exception-caught
                # Catch any possible errors generated by the connection to the API, generating the samplesheet or generating the run script
                log.error(f"Error for {run_name}: {e}")

        return 0

    def process_run(self, run_name: str, mode: DataTypeMode):
        """
        Per run processing
        """

        log.info(f"Processing: {run_name}")
        sample_count = 0

        # Create folder
        folder_path = os.path.join(self.target_dir, run_name)
        if self.samplesheet_only is False and not self.storage_interface.exists(folder_path):
            self.storage_interface.make_dirs(folder_path)

        # Samplesheet path
        samplesheet_path = os.path.join(folder_path, "samplesheet.csv")

        if self.use_api is False:
            # Write default samplesheet
            sample_count = 1
            with open(samplesheet_path, "w", encoding="UTF-8") as file:
                file.write("id,sample_name,group,user,project_id,project_limsid,project_type,reference_genome,data_analysis_type,barcode\n")
                file.write("sample_01,sample_01,asf,no_name,no_proj,no_lims_proj,no_type,no_ref,no_analysis,unclassified\n")
        if self.use_api is True:
            # Get samplesheet from API
            sample_dict = self.get_samplesheet(run_name, mode)

            # Write samplesheet
            sample_count, samplesheet = self.create_samplesheet(sample_dict)
            samplesheet_content = "\n".join(samplesheet) + "\n"
            self.storage_interface.write_file(samplesheet_path, samplesheet_content)

        # Set 666 for the samplesheet
        self.storage_interface.chmod(samplesheet_path, "rw-rw-rw-")

        # Generate and write sbatch script
        if self.samplesheet_only is False:
            # Detect muliplexed samples
            bc_parse_pos = -1
            if sample_count > 1:
                bc_parse_pos = 2

            # Create sbatch script
            sbatch_script = self.create_ont_sbatch_text(run_name, bc_parse_pos)
            sbatch_script_path = os.path.join(folder_path, "run_script.sh")
            self.storage_interface.write_file(sbatch_script_path, sbatch_script)

            # Set 777 for the run script
            self.storage_interface.chmod(sbatch_script_path, "rwxrwxrwx")

    def get_samplesheet(self, run_name: str, mode: DataTypeMode) -> dict:
        """
        Get samplesheet from API
        """
        # Get samplesheet from API
        api = ClarityHelperLims()

        # Check mode and set the appropriate check function
        if mode == DataTypeMode.ONT:
            sample_dict = api.collect_samplesheet_info(run_name)
        elif mode == DataTypeMode.ILLUMINA:
            # extract illumina RunId/flowcell name, then run check function
            iu = IlluminaUtils()
            run_dir = os.path.join(self.source_dir, run_name)
            illumina_run_name = iu.extract_illumina_runid_frompath(run_dir, "RunInfo.xml")
            sample_dict = api.collect_samplesheet_info(illumina_run_name)
        else:
            sample_dict = api.collect_samplesheet_info(run_name)
            log.warning(f"Mode selected: {mode}. General samplesheet created.")

        return sample_dict

    def create_samplesheet(self, sample_dict: dict) -> list:
        """
        Create a samplesheet as a list of strings
        """
        samplesheet = []
        count = 0
        samplesheet.append("id,sample_name,group,user,project_id,project_limsid,project_type,reference_genome,data_analysis_type,barcode")
        for key, value in sample_dict.items():
            # Convert all None values to "" for CSV
            clean_dict = {key: ("" if value is None or value == "None" else value) for key, value in value.items()}
            barcode = "unclassified"
            if "barcode" in value and clean_dict["barcode"] != "":
                barcode = clean_dict["barcode"]
            samplesheet.append(
                f"{key},{clean_dict['sample_name']},{clean_dict['group']},{clean_dict['user']},{clean_dict['project_id']},{clean_dict['project_limsid']},{clean_dict['project_type']},{clean_dict['reference_genome']},{clean_dict['data_analysis_type']},{barcode}"
            )
            count += 1

        return count, samplesheet

    def create_ont_sbatch_text(self, run_name: str, parse_pos: int = -1) -> str:
        """Creates an sbatch script from a template and returns the text

        Returns:
            str: Script as a string
        """

        # Create sbatch header
        header_str = create_sbatch_header(self.nextflow_version)

        # Create NXF_HOME string
        nxf_home = ""
        if self.nextflow_cache != "":
            nxf_home = f'export NXF_HOME="{self.nextflow_cache}"'

        # Create the bash script template with placeholders
        bash_script = f"""#!/bin/sh

#SBATCH --partition=ncpu
#SBATCH --job-name=asf_nanopore_demux_{run_name}
#SBATCH --mem=4G
#SBATCH -n 1
#SBATCH --time=168:00:00
#SBATCH --output=run.o
#SBATCH --error=run.o
#SBATCH --res=asf

{header_str}

{nxf_home}
export NXF_WORK="{self.nextflow_work}"
export NXF_SINGULARITY_CACHEDIR="{self.container_cache}"

nextflow run {self.pipeline_dir} \\
  -resume \\
  -profile crick,nemo \\
  --monochrome_logs \\
  --samplesheet ./samplesheet.csv \\
  --run_dir {os.path.join(self.runs_dir, run_name)} \\
  --dorado_model sup"""
        # Add parse pos if it is not -1
        if parse_pos != -1:
            bash_script += f" \\\n  --dorado_bc_parse_pos {parse_pos}\n"
        else:
            bash_script += "\n"

        return bash_script
