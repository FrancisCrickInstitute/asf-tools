"""
Function for managing CLI operation
"""

import csv
import io
import logging
import os

from asf_tools.api.clarity.clarity_helper_lims import ClarityHelperLims
from asf_tools.illumina.illumina_utils import extract_illumina_runid_frompath
from asf_tools.io.data_management import DataManagement, DataTypeMode
from asf_tools.io.storage_interface import StorageInterface
from asf_tools.nextflow.utils import create_sbatch_header


log = logging.getLogger(__name__)


def run_cli(
    api: ClarityHelperLims,
    storage_interface: StorageInterface,
    data_manager: DataManagement,
    data_type: DataTypeMode,
    source_dir: str,
    target_dir: str,
    run_name_contains: str,
    samplesheet_only: bool,
    use_api: bool,
    nextflow_version: str,
    nextflow_cache: str,
    nextflow_work: str,
    container_cache: str,
    pipeline_dir: str,
    runs_dir: str,
):
    log.debug("Scanning run folder")

    # Check for completed runs
    dir_diff = check_runs(storage_interface, data_manager, source_dir, target_dir, run_name_contains, samplesheet_only)
    log.info(f"Found {len(dir_diff)} completed runs")

    # Process runs
    for run_name in dir_diff:
        try:
            process_run(
                api,
                storage_interface,
                run_name,
                data_type,
                source_dir,
                target_dir,
                samplesheet_only,
                use_api,
                nextflow_version,
                nextflow_cache,
                nextflow_work,
                container_cache,
                pipeline_dir,
                runs_dir,
            )
        except Exception as e:  # pylint: disable=broad-exception-caught
            # Catch any possible errors generated by the connection to the API, generating the samplesheet or generating the run script
            log.error(f"Error for {run_name}: {e}")

    return 0


def check_runs(
    storage_interface: StorageInterface,
    data_manager: DataManagement,
    source_dir: str,
    target_dir: str,
    run_name_contains: str,
    samplesheet_only: bool,
) -> list:
    # Pull list of directory names
    source_dir_names = set(storage_interface.list_directories_with_links(source_dir))
    target_dir_names = set(storage_interface.list_directories_with_links(target_dir))

    # Check for a sequencing_summary file in source directory to show that the run is completed
    completed_runs = []
    for run_name in source_dir_names:
        run_dir = os.path.join(source_dir, run_name)
        completed_file_exists = data_manager.check_ont_sequencing_run_complete(run_dir)
        if completed_file_exists:
            completed_runs.append(run_name)
        else:
            log.debug(f"Skipping {run_name}: no summary file detected, run not completed.")
    source_dir_names = set(completed_runs)
    log.info(f"Found {len(completed_runs)} completed runs")

    # Get diff
    if samplesheet_only is False:
        dir_diff = source_dir_names - target_dir_names
        log.info(f"Found {len(dir_diff)} new run folders")
    else:
        dir_diff = target_dir_names
        log.info(f"Found {len(dir_diff)} existing run folders")

    # Filter for contains
    if run_name_contains is not None:
        dir_diff = [run for run in dir_diff if run_name_contains in run]
        log.info(f"Found {len(dir_diff)} new run folders after filtering for {run_name_contains}")

    return dir_diff


def check_runs_no_cli(storage_interface: StorageInterface, data_manager: DataManagement, source_dir: str, target_dir: str, max_date=None):
    log.info("Scanning run folder for new sequencing runs")

    # Pull list of directory names
    source_dir_names = set(storage_interface.list_directories_with_links(source_dir, max_date))
    target_dir_names = set(storage_interface.list_directories_with_links(target_dir))

    # Check for source dir_names that are not in target_dir_names
    dir_diff = source_dir_names - target_dir_names

    # Sort the list of directory names
    dir_diff = sorted(dir_diff)

    # Check for a sequencing_summary file in source directory to show that the run is completed
    completed_runs = []
    for run_name in dir_diff:
        run_dir = os.path.join(source_dir, run_name)
        completed_file_exists = data_manager.check_ont_sequencing_run_complete(run_dir)
        if completed_file_exists:
            completed_runs.append(run_name)
        else:
            log.debug(f"Skipping {run_name}: no summary file detected, run not completed.")
    log.info(f"Found {len(completed_runs)} completed runs")

    return completed_runs


def process_run(
    api: ClarityHelperLims,
    storage_interface: StorageInterface,
    run_name: str,
    mode: DataTypeMode,
    source_dir: str,
    target_dir: str,
    samplesheet_only: bool,
    use_api: bool,
    nextflow_version: str,
    nextflow_cache: str,
    nextflow_work: str,
    container_cache: str,
    pipeline_dir: str,
    run_file_runs_dir: str,
) -> None:
    """
    Per run processing
    """

    log.info(f"Processing: {run_name}")
    sample_count = 0

    # Create folder
    folder_path = os.path.join(target_dir, run_name)
    if samplesheet_only is False and not storage_interface.exists(folder_path):
        storage_interface.make_dirs(folder_path)

    # Samplesheet path
    samplesheet_path = os.path.join(folder_path, "samplesheet.csv")

    if use_api is False:
        # Write default samplesheet
        sample_count = 1
        with open(samplesheet_path, "w", encoding="UTF-8") as file:
            file.write("id,sample_name,group,user,project_id,project_limsid,project_type,reference_genome,data_analysis_type,barcode\n")
            file.write("sample_01,sample_01,asf,no_name,no_proj,no_lims_proj,no_type,no_ref,no_analysis,unclassified\n")
    if use_api is True:
        # Get samplesheet from API
        sample_dict = get_samplesheet(api, run_name, mode, source_dir)

        # Write samplesheet
        sample_count, samplesheet = create_samplesheet(sample_dict)
        samplesheet_content = "\n".join(samplesheet) + "\n"
        storage_interface.write_file(samplesheet_path, samplesheet_content)

    # Set 666 for the samplesheet
    storage_interface.chmod(samplesheet_path, "rw-rw-rw-")

    # Generate and write sbatch script
    if samplesheet_only is False:
        # Detect muliplexed samples
        bc_parse_pos = -1
        if sample_count > 1:
            bc_parse_pos = 9

        # Extract pipeline parameters
        pipeline_params = None
        if use_api is True:
            pipeline_params = extract_pipeline_params(api, storage_interface, samplesheet_path)

        # Create sbatch script
        sbatch_script = create_ont_sbatch_text(
            run_name,
            pipeline_params,
            nextflow_version,
            nextflow_cache,
            nextflow_work,
            container_cache,
            pipeline_dir,
            run_file_runs_dir,
            parse_pos=bc_parse_pos,
        )
        sbatch_script_path = os.path.join(folder_path, "run_script.sh")
        storage_interface.write_file(sbatch_script_path, sbatch_script)

        # Set 777 for the run script
        storage_interface.chmod(sbatch_script_path, "rwxrwxrwx")


def get_samplesheet(api, run_name: str, mode: DataTypeMode, source_dir: str) -> dict:
    """
    Get samplesheet from API
    """
    # Get samplesheet from API

    # Check mode and set the appropriate check function
    if mode == DataTypeMode.ONT:
        sample_dict = api.collect_samplesheet_info(run_name, True)
    elif mode == DataTypeMode.ILLUMINA:
        # extract illumina RunId/flowcell name, then run check function
        run_dir = os.path.join(source_dir, run_name)
        illumina_run_name = extract_illumina_runid_frompath(run_dir, "RunInfo.xml")
        sample_dict = api.collect_samplesheet_info(illumina_run_name)
    else:
        sample_dict = api.collect_samplesheet_info(run_name)
        log.warning(f"Mode selected: {mode}. General samplesheet created.")

    return sample_dict


def create_samplesheet(sample_dict: dict) -> list:
    """
    Create a samplesheet as a list of strings
    """
    samplesheet = []
    count = 0
    samplesheet.append("id,sample_name,group,user,project_id,project_limsid,project_type,reference_genome,data_analysis_type,barcode")
    for key, value in sample_dict.items():
        # Convert all None values to "" for CSV
        clean_dict = {key: ("" if value is None or value == "None" else value) for key, value in value.items()}
        barcode = "unclassified"
        if "barcode" in value and clean_dict["barcode"] != "":
            barcode = clean_dict["barcode"]
        samplesheet.append(
            f"{key},{clean_dict['sample_name']},{clean_dict['group']},{clean_dict['user']},{clean_dict['project_id']},{clean_dict['project_limsid']},{clean_dict['project_type']},{clean_dict['reference_genome']},{clean_dict['data_analysis_type']},{barcode}"
        )
        count += 1

    return count, samplesheet


def extract_pipeline_params(api, storage_interface, samplesheet_csv) -> dict:
    """
    Extracts the first project IDs from a CSV file and retrieves the corresponding pipeline parameters.
    """
    params_dict = {}

    # Read the file
    sample_sheet = storage_interface.read_file(samplesheet_csv)
    reader = csv.DictReader(io.StringIO(sample_sheet))

    # Check there's a column for "project_id"
    if "project_id" not in reader.fieldnames:
        return params_dict

    # Extract the first project ID
    first_row = next(reader, None)
    proj_id = first_row["project_id"]
    params_dict = api.get_pipeline_params(proj_id, "pipeline params", "=")

    return params_dict


def create_ont_sbatch_text(
    run_name: str,
    pipeline_params_dict: dict,
    nextflow_version: str,
    nextflow_cache: str,
    nextflow_work: str,
    container_cache: str,
    pipeline_dir: str,
    run_file_runs_dir: str,
    parse_pos: int = -1,
) -> str:
    """Creates an sbatch script from a template and returns the text

    Returns:
        str: Script as a string
    """

    # Create sbatch header
    header_str = create_sbatch_header(nextflow_version)

    # Create NXF_HOME string
    nxf_home = ""
    if nextflow_cache != "":
        nxf_home = f'export NXF_HOME="{nextflow_cache}"'

    # Create the bash script template with placeholders
    bash_script = f"""#!/bin/sh

#SBATCH --partition=ncpu
#SBATCH --job-name=asf_nanopore_demux_{run_name}
#SBATCH --mem=4G
#SBATCH -n 1
#SBATCH --time=168:00:00
#SBATCH --output=run.o
#SBATCH --error=run.o
#SBATCH --res=asf

{header_str}

{nxf_home}
export NXF_WORK="{nextflow_work}"
export NXF_SINGULARITY_CACHEDIR="{container_cache}"

nextflow run {pipeline_dir} \\
  -resume \\
  -profile crick,genomics \\
  --monochrome_logs \\
  --samplesheet ./samplesheet.csv \\
  --run_dir {os.path.join(run_file_runs_dir, run_name)} \\
  --dorado_model sup"""
    # Add pipeline params
    if pipeline_params_dict:
        param_lines = [f"  --{key} {value}" for param in pipeline_params_dict.values() for key, value in param.items()]

        if param_lines:
            bash_script += " \\\n"  # Add "\" only if parameters exist
            bash_script += " \\\n".join(param_lines)

    # Add parse pos if it is not -1
    if parse_pos != -1:
        bash_script += f" \\\n  --dorado_bc_parse_pos {parse_pos}\n"

    bash_script += "\n"

    return bash_script
